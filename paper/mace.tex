\section{Proposed Batch Bayesian Optimization Algorithm}
We will present the proposed batch Bayesian optimization algorithm in this section.

\begin{figure*}[tp!]
    \centering
    \subfigure[True phase margin curve, GP predictions, and Pareto set]{\label{fig:PF_example_1}\includegraphics[width=0.49\textwidth]{./img/pm_gp_ps.eps}}
    ~
    \subfigure[The LCB, EI, and PI functions and the Pareto front]{\label{fig:PF_example_2}\includegraphics[width=0.49\textwidth]{./img/pf.eps}}
    \caption{Illustration of the multi-objective optimization of acquisition functions}
    \label{fig:PF_example}
\end{figure*}

\subsection{Multi-objective Optimization}\label{sec:MOForumlation}

Unlike single-objective optimization, there are multiple objectives to optimize in multi-objective optimization problems\cite{MO_overview}. The multi-objective optimization problem is formulated as
\begin{equation}
    \label{eq:MOFormulation}
    \begin{aligned}
        & \text{minimize} & & f_1(\bm{x}),~\dots~,f_m(\bm{x}).
    \end{aligned}
\end{equation}
The multiple objectives to be optimized can be conflicting so that it is usually impossible to find a single solution that is the optimum of all objectives. The goal of multi-objective optimization algorithms is to approximate the \emph{Pareto front} of the objectives. A solution $\bm{x}_1$ is said to \emph{dominate} $\bm{x}_2$ if $\forall i \in \{1\dots m\},~f_i(\bm{x}_1) \le f_i(\bm{x}_2)$ and $\exists j \in \{1\dots m\}, f_j(\bm{x}_1) < f_j(\bm{x}_2)$. A design is \emph{Pareto-optimal} if it is not dominated by any other point in the design space and dominates at least one point. The whole set of the Pareto-optimal points in the design space is called the \emph{Pareto set}, and the set of Pareto-optimal points in the objective space is called the \emph{Pareto front}. It is often unlikely to get the whole Pareto front as there might be infinite points on the Paret front, multi-objective optimization algorithms try to find a set of evenly distributed solutions that approximate the true Pareto front.

There exist many mature multi-objective optimization algorithms, like the non-dominated sorting based genetic algorithm (NSGA-II)~\cite{nsgaii}, and the multi-objective evolutionary algorithm based on decomposition (MOEA/D)~\cite{moead}. In this paper, the multi-objective optimization based on differential evolution (DEMO)~\cite{demo} is used to solve multi-objective optimization problems, but other multi-objective optimization algorithms can also be applied.

\subsection{Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble}

Each acquisition function represents a unique selection strategy, different acquisition functions may not agree with each other about where to sample the next point. For example, the value of LCB function always decreases as the $\sigma(\bm{x})$ increases. However, for the PI function, when $\sigma(\bm{x})$ increases, the value of PI would decrease when $\mu(\bm{x}) < \tau$, and increase when $\mu(\bm{x}) > \tau$. For the EI function, the values of EI function at already sampled points would always be lower than the values at any unsampled locations, while this property does not hold for the LCB function.

\begin{figure*}[!htb]
    \begin{center}
        \centerline{\includegraphics[width=1.0\linewidth]{./img/convplot.eps}}
        \caption{Optimization results of the benchmark functions}
        \label{fig:CovPlotBenchmark}
    \end{center}
    \vskip -0.15in
\end{figure*}

\begin{algorithm}[!htb]
    \caption{Multi-objective Acquisition Ensemble Algorithm}
    \label{alg:MACE}
    \begin{algorithmic}[1]
        \REQUIRE Number of initial sampling points $N_{init}$, number of iterations $N_{iter}$, batch size $B$.
        \STATE Randomly sample $N_{init}$ points in the design space
        \STATE Construct initial GP model
        \FOR{t = 1, 2, \dots, $N_{iter}$}
        \STATE Construct the LCB, EI and PI functions according to \eqref{eq:LCB} and \eqref{eq:PI_EI}
        \STATE Find the Pareto front of LCB, EI, PI functions using the DEMO algorithm
        \STATE Randomly sample $B$ points $\bm{x}_1, \dots, \bm{x}_B$ from the Pareto-optimal points
        \STATE Evaluate $\bm{x}_1, \dots, \bm{x}_B$ to get $y_1 = f(\bm{x}_1),~\dots~,y_B = f(\bm{x}_B)$
        \STATE Update the GP model
        \ENDFOR
        \STATE \textbf{Return} best $f(\bm{x})$ recorded during iterations
    \end{algorithmic}
\end{algorithm}


With multi-objective optimization, the best trade-off between acquisition functions can be captured by the Pareto front of these acquisition functions. We can then sample on the Pareto front to obtain multiple candidate points for the objective function evaluations.

The proposed MACE algorithm is described in Algorithm~\ref{alg:MACE}. In the proposed MACE algorithm, the LCB, EI, and PI acquisition functions are selected. Other acquisition functions like KG and PES can also be incorporated into the MACE framework. In each iteration, the following multi-objective optimization problem is constructed:
\begin{equation}
    \label{eq:MO_LCB_EI_PI}
    \begin{aligned}
        & \text{minimize} & & \mathrm{LCB}(\bm{x}),~-\mathrm{EI}(\bm{x}),~-\mathrm{PI}(\bm{x}).
    \end{aligned}
\end{equation}
Then the DEMO multi-objective optimization algorithm~\cite{demo} is applied to solve the multi-objective problem in \eqref{eq:MO_LCB_EI_PI}. Once the Pareto front of LCB, EI and PI is obtained, the candidate points are then randomly sampled from the Pareto front.

In Figure~\ref{fig:PF_example}, we illustrate the proposed MACE algorithm using an example of a real-world amplifier circuit. The optimization objective is to maximize the phase margin (PM) of the amplifier, so the FOM is defined as $\mathrm{FOM}(\bm{x}) = - \mathrm{PM}(\bm{x})$. The width of one of its transistor is the design variable. We sweep the width of the transistor and perform HSPICE simulations to get the FOM values. The curve of FOM values is plotted in Figure~\ref{fig:PF_example_1} (the blue line). Several points are randomly sampled from the FOM curve to train the GP model. The LCB, EI, PI functions and the Pareto front of the acquisition functions are plotted in Figure~\ref{fig:PF_example_2}. We can see from Figure~\ref{fig:PF_example_2} that the optimal locations of the three acquisition functions are different, while their best trade-off is captured by the Pareto front. The Pareto set that represents the best trade-off between the three acquisition functions is the interval $[43, 50.4]$, as plotted in Figure~\ref{fig:PF_example_1}. The candidate points for the next batch of evaluations are randomly sampled from the Pareto set.

