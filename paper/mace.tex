\section{Proposed Batched Bayesian Optimization Algorithm}

\subsection{Multi-objective Optimization of Acquisition Functions}\label{sec:MOForumlation}

\textcolor{red}{See how other MOBO paper formulate multi-objective optimization}

Unlike single-objective optimization, multi-objective optimization problems
simultaneously optimize multiple objectives~\cite{MO_overview}, the
multi-objective optimization problem is formulated as
\begin{equation}
    \label{eq:MOFormulation}
    \begin{aligned}
        & \text{minimize} & & f_1(\bm{x}),~\dots~,f_m(\bm{x})
    \end{aligned}
\end{equation}
When we have multiple objectives to optimizes, the objectives can be conflicting, so that it is usually impossible to find a single solution that is the optimum of all objectives, the goal of multi-objective optimization algorithms is to appropriate the \emph{Pareto front} of the objectives. A solution $\bm{x}_1$ is said to \emph{dominate} $\bm{x}_2$ if $\forall i \in \{1\dots m\},~f_i(\bm{x}_1) \le f_i(\bm{x}_2)$ and $\exists j \in \{1\dots m\}, f_j(\bm{x}_1) < f_j(\bm{x}_2)$. A design is \emph{Pareto-optimal} if it is not dominated by any other point in the design space and dominate at least one point. The whole set of the Pareto-optimal points in the design space is called the \emph{Pareto set}, and the set of Pareto-optimal points in the objective space is called the \emph{Pareto front}. It is often unlikely to get the whole Pareto front as there might be infinite points on the front, the goal of multi-objective optimization is to find a set of designs that approximates the true Pareto front.

There exist many mature multi-objective optimization algorithms, like the
non-dominated sorting based genetic algorithm (NSGA-II)~\cite{nsgaii}, and the
multi-objective evolutionary algorithm based on decomposition
(MOEA/D)~\cite{moead}. In this paper, the multi-objective optimization based on
differential evolution (DEMO)~\cite{demo} is used to solve multi-objective
optimization problems.

% When we have more than one objective to optimize, the problem is formulated as:

\subsection{Batched Bayesian Optimization via Multi-objective Acquisition Ensemble}

We propose a novel heuristic for the parallelization of Bayesian optimization.
The parallelization is realized via the multi-objective ensemble of multiple
acquisition functions. As each acquisition function represents a unique
interpretation to the optimization problem, different acquisition functions may
not agree with each other about where to sample the next point. For example,
the value of LCB function always decreases as the $\sigma(\bm{x})$ increases,
however, for the PI function, when $\sigma(\bm{x})$ increases, the value of PI
would decrease when $\mu(\bm{x}) < \tau$, and increase when $\mu(\bm{x}) >
\tau$. For a noise-free problem, the EI values at already sampled points are
always zero, so that any unsampled point would have a better EI value than the
sampled points, while this property does not hold for the LCB function.

\textcolor{red}{TODO: Illustrate PF}
\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\columnwidth]{./img/pf.jpg}}
        \caption{Illustration of Pareto front}
        \label{fig:PF_example}
    \end{center}
    \vskip -0.2in
\end{figure}

With multi-objective optimization, the best trade-off between acquisition functions can be captured by the Pareto front of these acquisition functions. We can then sample on the Pareto front to obtain multiple candidate points for the objective function evaluations. 

In the proposed MACE algorithm, the LCB, EI, and PI acquisition functions are selected, but other acquisition functions like KG and PES can also be incorporated into the MACE framework. In each iteration, the following MO problem is constructed:
\begin{equation}
    \label{eq:MO_LCB_EI_PI}
    \begin{aligned}
        & \text{minimize} & & \mathrm{LCB}(\bm{x}),~-\mathrm{EI}(\bm{x}),~-\mathrm{PI}(\bm{x}).
    \end{aligned}
\end{equation}
Then the DEMO multi-objective optimization algorithm is applied to solve the multi-objective problem in \eqref{eq:MO_LCB_EI_PI}. Once the Pareto front of LCB, EI and PI is obtained, the candidate evalution points are then randomly sampled from the Pareto front.

% TODO: We illustrate the proposed MACE algorithm using the Branin-Hoo function~\cite{dixon1978global}
% TODO: plot contour of Branin/Ei/LCB/PI, plot the PS and the PF

The proposed MACE algorithm is described in Algorithm~\ref{alg:MACE}.

\textcolor{red}{TODO: Input, Output parameters, see how other papers format the algorithm description}
\begin{algorithm}
\caption{Multi-objective Acquisition Ensemble Algorithm}
\label{alg:MACE}
\begin{algorithmic}[1]
\STATE Initial Sampling
\STATE Construct initial GP model
\FOR{t = 1, 2, \dots}
    \STATE Construct the LCB, EI and PI functions according to \eqref{eq:LCB} and \eqref{eq:PI_EI}
    \STATE Find the Pareto front of LCB, EI, PI function using the DEMO algorithm
    \STATE Randomly sample B points $\bm{x}_1, \dots, \bm{x}_B$ from the Pareto front where B is the batch size
    \STATE Evaluate $\bm{x}_1, \dots, \bm{x}_B$ to get $y_1 = f(\bm{x}_1),~\dots~,y_B = f(\bm{x}_B)$
    \STATE Update the GP model
\ENDFOR
\STATE Return best $f(\bm{x})$ recorded during iterations
\end{algorithmic}
\end{algorithm}

\textcolor{red}{TODO: If there is space, discuss the approximation of EI in a new subsection "Implementation details"}
